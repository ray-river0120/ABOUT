{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ray-river0120/ABOUT/blob/main/DAEN_690_crawl4ai_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cba38e5",
      "metadata": {
        "id": "0cba38e5"
      },
      "source": [
        "#  üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scrapper\n",
        "<div align=\"center\">\n",
        "\n",
        "<a href=\"https://trendshift.io/repositories/11716\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11716\" alt=\"unclecode%2Fcrawl4ai | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n",
        "\n",
        "[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)\n",
        "[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)\n",
        "\n",
        "[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)\n",
        "[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)\n",
        "[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)\n",
        "\n",
        "[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)\n",
        "\n",
        "</div>\n",
        "\n",
        "Crawl4AI simplifies asynchronous web crawling and data extraction, making it accessible for large language models (LLMs) and AI applications. üÜìüåê\n",
        "\n",
        "- GitHub Repository: [https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n",
        "- Twitter: [@unclecode](https://twitter.com/unclecode)\n",
        "- Website: [https://crawl4ai.com](https://crawl4ai.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41de6458",
      "metadata": {
        "id": "41de6458"
      },
      "source": [
        "### **Quickstart with Crawl4AI**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1380e951",
      "metadata": {
        "id": "1380e951"
      },
      "source": [
        "#### 1. **Installation**\n",
        "Install Crawl4AI and necessary dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fecfad",
      "metadata": {
        "id": "05fecfad"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U crawl4ai\n",
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check crawl4ai version\n",
        "import crawl4ai\n",
        "print(crawl4ai.__version__.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSFh3BtZdfb6",
        "outputId": "8457788c-0a1b-4809-f23d-eda098f62b99"
      },
      "id": "ZSFh3BtZdfb6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4.248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Setup Crawl4ai\n",
        "The following command installs Playride and its dependencies and updates a few configurations for Crawl4ai. After that, you can run the doctor command to ensure everything works as it should.\n"
      ],
      "metadata": {
        "id": "JJE1jyrol8hn"
      },
      "id": "JJE1jyrol8hn"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!crawl4ai-setup"
      ],
      "metadata": {
        "id": "31TTKe7AhhpK",
        "collapsed": true
      },
      "id": "31TTKe7AhhpK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-doctor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpxycFlXYJOb",
        "outputId": "650d484c-30d6-4519-e37d-59d1c0ea0b06"
      },
      "id": "fpxycFlXYJOb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Running Crawl4AI health check...\n",
            "[INIT].... ‚Üí Crawl4AI 0.4.248\n",
            "[TEST].... ‚Ñπ Testing crawling capabilities...\n",
            "[EXPORT].. ‚Ñπ Exporting PDF and taking screenshot took 1.97s\n",
            "[FETCH]... ‚Üì https://crawl4ai.com... | Status: True | Time: 4.50s\n",
            "[SCRAPE].. ‚óÜ Processed https://crawl4ai.com... | Time: 75ms\n",
            "[COMPLETE] ‚óè https://crawl4ai.com... | Status: True | Total: 4.58s\n",
            "[COMPLETE] ‚óè ‚úÖ Crawling test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you face with an error try it manually\n",
        "# !playwright install --with-deps chrome # Recommended for Colab/Linux"
      ],
      "metadata": {
        "id": "i-uP1YRMYRpC"
      },
      "id": "i-uP1YRMYRpC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I suggest you first try the code below to ensure that Playwright is installed and works properly."
      ],
      "metadata": {
        "id": "Af4BY4Zvlf0D"
      },
      "id": "Af4BY4Zvlf0D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2a74c8",
      "metadata": {
        "id": "2c2a74c8"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def test_browser():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto('https://example.com')\n",
        "        print(f'Title: {await page.title()}')\n",
        "        await browser.close()\n",
        "\n",
        "asyncio.run(test_browser())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by3AVeSwlcba",
        "outputId": "e1a6a6ea-d36b-4e34-c496-fa18abc1c734"
      },
      "id": "by3AVeSwlcba",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Example Domain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c558d7",
      "metadata": {
        "id": "f3c558d7"
      },
      "source": [
        "#### 2. **Basic Setup and Simple Crawl**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003376f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "003376f3",
        "outputId": "9c390343-f342-4632-93cd-7e56cac119c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.248\n",
            "[FETCH]... ‚Üì https://www.kidocode.com/degrees/technology... | Status: True | Time: 2.02s\n",
            "[SCRAPE].. ‚óÜ Processed https://www.kidocode.com/degrees/technology... | Time: 69ms\n",
            "[COMPLETE] ‚óè https://www.kidocode.com/degrees/technology... | Status: True | Total: 2.12s\n",
            "[![coding school for kids](https://cdn.prod.website-files.com/61d6943d6b5924685ac825ca/64a6a12136e8f756c9df3baa_k-combomark-white.svg)](https://www.kidocode.com/degrees/</>) -- [Trial Class](https://www.kidocode.com/degrees/</trial-class>) -- Degrees -- degrees -- [All Degrees](https://www.kidocode.com/degrees/</degrees>) -- [AI Degree](https://www.kidocode.com/degrees/</degrees/artificial-intelligence>) -- [Technology Degree](https://www.kidocode.com/degrees/</degrees/technology>) -- [Entrepreneurship Degree](https\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from crawl4ai import AsyncWebCrawler, CacheMode, BrowserConfig, CrawlerRunConfig, CacheMode\n",
        "\n",
        "async def simple_crawl():\n",
        "    crawler_run_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS)\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.kidocode.com/degrees/technology\",\n",
        "            config=crawler_run_config\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print the first 500 characters\n",
        "\n",
        "asyncio.run(simple_crawl())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da9b4d50",
      "metadata": {
        "id": "da9b4d50"
      },
      "source": [
        "#### 3. **Dynamic Content Handling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb8c1e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb8c1e4",
        "outputId": "3185a892-d1ce-4006-e8d1-2dd204c3ee1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.248\n",
            "[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 7.84s\n",
            "[SCRAPE].. ‚óÜ Processed https://www.nbcnews.com/business... | Time: 432ms\n",
            "[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 8.34s\n",
            "IE 11 is not supported. For an optimal experience visit our site on another browser. -- [BREAKING: An 86-year-old white man accepts a plea deal for wrong-door shooting of Black teenager Ralph Yarl ](https://www.nbcnews.com/<https:/www.nbcnews.com/news/us-news/plea-deal-reached-wrong-door-shooting-teenager-ralph-yarl-rcna192042>)[](https://www.nbcnews.com/<https:/www.nbcnews.com/news/us-news/plea-deal-reached-wrong-door-shooting-teenager-ralph-yarl-rcna192042>) -- Skip to Content -- [NBC News Logo](https:\n"
          ]
        }
      ],
      "source": [
        "async def crawl_dynamic_content():\n",
        "    # You can use wait_for to wait for a condition to be met before returning the result\n",
        "    # wait_for = \"\"\"() => {\n",
        "    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;\n",
        "    # }\"\"\"\n",
        "\n",
        "    # wait_for can be also just a css selector\n",
        "    # wait_for = \"article.tease-card:nth-child(10)\"\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        js_code = [\n",
        "            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n",
        "        ]\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,\n",
        "            js_code=js_code,\n",
        "            # wait_for=wait_for,\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            config=config,\n",
        "\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print first 500 characters\n",
        "\n",
        "asyncio.run(crawl_dynamic_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86febd8d",
      "metadata": {
        "id": "86febd8d"
      },
      "source": [
        "#### 4. **Content Cleaning and Fit Markdown**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8ab01f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e8ab01f",
        "outputId": "3fd48d12-f446-4fdf-caef-f524c1fb0869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.248\n",
            "[FETCH]... ‚Üì https://en.wikipedia.org/wiki/Apple... | Status: True | Time: 2.29s\n",
            "[SCRAPE].. ‚óÜ Processed https://en.wikipedia.org/wiki/Apple... | Time: 2152ms\n",
            "[COMPLETE] ‚óè https://en.wikipedia.org/wiki/Apple... | Status: True | Total: 4.53s\n",
            "Full Markdown Length: 89369\n",
            "Fit Markdown Length: 73113\n"
          ]
        }
      ],
      "source": [
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "async def clean_content():\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,\n",
        "            excluded_tags=['nav', 'footer', 'aside'],\n",
        "            remove_overlay_elements=True,\n",
        "            markdown_generator=DefaultMarkdownGenerator(\n",
        "                content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0),\n",
        "                options={\n",
        "                    \"ignore_links\": True\n",
        "                }\n",
        "            ),\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://en.wikipedia.org/wiki/Apple\",\n",
        "            config=config,\n",
        "        )\n",
        "        full_markdown_length = len(result.markdown_v2.raw_markdown)\n",
        "        fit_markdown_length = len(result.markdown_v2.fit_markdown)\n",
        "        print(f\"Full Markdown Length: {full_markdown_length}\")\n",
        "        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n",
        "\n",
        "\n",
        "asyncio.run(clean_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55715146",
      "metadata": {
        "id": "55715146"
      },
      "source": [
        "#### 5. **Link Analysis and Smart Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def link_analysis():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,\n",
        "            exclude_external_links=True,\n",
        "            exclude_social_media_links=True,\n",
        "            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            config=config,\n",
        "        )\n",
        "        print(f\"Found {len(result.links['internal'])} internal links\")\n",
        "        print(f\"Found {len(result.links['external'])} external links\")\n",
        "\n",
        "        for link in result.links['internal'][:5]:\n",
        "            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n",
        "\n",
        "\n",
        "asyncio.run(link_analysis())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOScej33I86q",
        "outputId": "bb75e2d1-2a42-4901-ec0e-a11017a7494a"
      },
      "id": "tOScej33I86q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... ‚Üí Crawl4AI 0.4.248\n",
            "[FETCH]... ‚Üì https://www.nbcnews.com/business... | Status: True | Time: 0.05s\n",
            "[COMPLETE] ‚óè https://www.nbcnews.com/business... | Status: True | Total: 0.10s\n",
            "Found 146 internal links\n",
            "Found 34 external links\n",
            "Href: https://www.nbcnews.com/news/us-news/plea-deal-reached-wrong-door-shooting-teenager-ralph-yarl-rcna192042\n",
            "Text: BREAKING: An 86-year-old white man accepts a plea deal for wrong-door shooting of Black teenager Ralph Yarl\n",
            "\n",
            "Href: https://www.nbcnews.com\n",
            "Text: NBC News Logo\n",
            "\n",
            "Href: https://www.nbcnews.com/watch#philadelphia\n",
            "Text: Live: Eagles parade\n",
            "\n",
            "Href: https://www.nbcnews.com/politics/politics-news/live-blog/live-updates-trump-tariffs-ukraine-russia-rcna191174\n",
            "Text: Trump Admin\n",
            "\n",
            "Href: https://www.nbcnews.com/politics\n",
            "Text: Politics\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80cceef3",
      "metadata": {
        "id": "80cceef3"
      },
      "source": [
        "#### 6. **Media Handling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fed7f99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fed7f99",
        "outputId": "a63f7125-fcfa-47cb-c95b-22f4d9185729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INIT].... ‚Üí Crawl4AI 0.4.248\n",
            "[FETCH]... ‚Üì https://www.freepik.com/free-photos-vectors/vape-s... | Status: True | Time: 0.04s\n",
            "[COMPLETE] ‚óè https://www.freepik.com/free-photos-vectors/vape-s... | Status: True | Total: 0.09s\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_3.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_11.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_14.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_5.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_2.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_15.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_18.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_12.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_4.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_7.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_9.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_19.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_1.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_20.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_6.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_13.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_8.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_10.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_16.jpg\n",
            "‚úÖ Saved: /content/drive/My Drive/Webscraping_data/Images/image_17.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')  # Gives Colab access to Google Drive\n",
        "\n",
        "# Step 2: Define the save directory in Google Drive\n",
        "save_dir = \"/content/drive/My Drive/Webscraping_data/Images\"\n",
        "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "async def download_image(session, img_url, img_path):\n",
        "    \"\"\"Download an image asynchronously and save it to Google Drive.\"\"\"\n",
        "    try:\n",
        "        async with session.get(img_url) as response:  # Send async request to get image\n",
        "            if response.status == 200:  # Check if request was successful\n",
        "                with open(img_path, \"wb\") as f:  # Open file in write-binary mode\n",
        "                    f.write(await response.read())  # Write image data to file\n",
        "                print(f\"Saved: {img_path}\")  # Print success message\n",
        "            else:\n",
        "                print(f\"‚ùå Failed to download: {img_url} (Status: {response.status})\")  # Print failure message\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error downloading {img_url}: {e}\")  # Handle download errors\n",
        "\n",
        "async def media_handling():\n",
        "    \"\"\"Scrape and download images from Freepik into Google Drive.\"\"\"\n",
        "    async with AsyncWebCrawler() as crawler:  # Start async web crawler\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.ENABLED,  # Enable cache mode for efficiency\n",
        "            exclude_external_images=False,  # Allow downloading external images\n",
        "        )\n",
        "        result = await crawler.arun(  # Run the crawler on Freepik\n",
        "            url=\"https://www.freepik.com/free-photos-vectors/vape-shop-marketing\",\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:  # Create async session for downloading\n",
        "            tasks = []  # List to store async tasks\n",
        "            for i, img in enumerate(result.media['images'][:20]):  # Loop through first 20 images\n",
        "                img_url = img['src']  # Extract image URL\n",
        "                img_name = f\"image_{i+1}.jpg\"  # Define image filename\n",
        "                img_path = os.path.join(save_dir, img_name)  # Define full save path\n",
        "\n",
        "                # Schedule an async image download task\n",
        "                tasks.append(download_image(session, img_url, img_path))\n",
        "\n",
        "            await asyncio.gather(*tasks)  # Run all image downloads in parallel\n",
        "\n",
        "# Step 3: Run the async function\n",
        "asyncio.run(media_handling())  # Execute the media_handling function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9290499a",
      "metadata": {
        "id": "9290499a"
      },
      "source": [
        "#### 7. **Using Hooks for Custom Workflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d069c2b",
      "metadata": {
        "id": "9d069c2b"
      },
      "source": [
        "Hooks in Crawl4AI allow you to run custom logic at specific stages of the crawling process. This can be invaluable for scenarios like setting custom headers, logging activities, or processing content before it is returned. Below is an example of a basic workflow using a hook, followed by a complete list of available hooks and explanations on their usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4d2fc8",
      "metadata": {
        "id": "bc4d2fc8"
      },
      "outputs": [],
      "source": [
        "async def before_goto(page: Page, context: BrowserContext, url: str, **kwargs):\n",
        "        \"\"\"Hook called before navigating to each URL\"\"\"\n",
        "        print(f\"[HOOK] before_goto - About to visit: {url}\")\n",
        "        # Example: Add custom headers for the request\n",
        "        await page.set_extra_http_headers({\n",
        "            \"Custom-Header\": \"my-value\"\n",
        "        })\n",
        "        return page\n",
        "\n",
        "async def custom_hook_workflow(verbose=True):\n",
        "    async with AsyncWebCrawler(config=BrowserConfig( verbose=verbose)) as crawler:\n",
        "        # Set a 'before_goto' hook to run custom code just before navigation\n",
        "        crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n",
        "\n",
        "        # Perform the crawl operation\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://crawl4ai.com\",\n",
        "            config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))\n",
        "\n",
        "asyncio.run(custom_hook_workflow())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff45e21",
      "metadata": {
        "id": "3ff45e21"
      },
      "source": [
        "List of available hooks and examples for each stage of the crawling process:\n",
        "\n",
        "- **on_browser_created**\n",
        "    ```python\n",
        "    async def on_browser_created_hook(browser):\n",
        "        print(\"[Hook] Browser created\")\n",
        "    ```\n",
        "\n",
        "- **before_goto**\n",
        "    ```python\n",
        "    async def before_goto_hook(page, context = None):\n",
        "        await page.set_extra_http_headers({\"X-Test-Header\": \"test\"})\n",
        "    ```\n",
        "\n",
        "- **after_goto**\n",
        "    ```python\n",
        "    async def after_goto_hook(page, context = None):\n",
        "        print(f\"[Hook] Navigated to {page.url}\")\n",
        "    ```\n",
        "\n",
        "- **on_execution_started**\n",
        "    ```python\n",
        "    async def on_execution_started_hook(page, context = None):\n",
        "        print(\"[Hook] JavaScript execution started\")\n",
        "    ```\n",
        "\n",
        "- **before_return_html**\n",
        "    ```python\n",
        "    async def before_return_html_hook(page, html, context = None):\n",
        "        print(f\"[Hook] HTML length: {len(html)}\")\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d56ebb1",
      "metadata": {
        "id": "2d56ebb1"
      },
      "source": [
        "#### 8. **Session-Based Crawling**\n",
        "\n",
        "When to Use Session-Based Crawling:\n",
        "Session-based crawling is especially beneficial when navigating through multi-page content where each page load needs to maintain the same session context. For instance, in cases where a ‚ÄúNext Page‚Äù button must be clicked to load subsequent data, the new data often replaces the previous content. Here, session-based crawling keeps the browser state intact across each interaction, allowing for sequential actions within the same session. An easy way to think about a session is that it acts like a browser tab; when you pass the same session ID, it uses the same browser tab and does not create a new tab.\n",
        "\n",
        "Example: Multi-Page Navigation Using JavaScript\n",
        "In this example, we‚Äôll navigate through multiple pages by clicking a \"Next Page\" button. After each page load, we extract the new content and repeat the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bfebae",
      "metadata": {
        "id": "e7bfebae"
      },
      "outputs": [],
      "source": [
        "from crawl4ai.extraction_strategy import (\n",
        "    JsonCssExtractionStrategy,\n",
        "    LLMExtractionStrategy,\n",
        ")\n",
        "import json\n",
        "\n",
        "async def crawl_dynamic_content_pages_method_2():\n",
        "    print(\"\\n--- Advanced Multi-Page Crawling with JavaScript Execution ---\")\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n",
        "        session_id = \"typescript_commits_session\"\n",
        "        all_commits = []\n",
        "        last_commit = \"\"\n",
        "\n",
        "        js_next_page_and_wait = \"\"\"\n",
        "        (async () => {\n",
        "            const getCurrentCommit = () => {\n",
        "                const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n",
        "                return commits.length > 0 ? commits[0].textContent.trim() : null;\n",
        "            };\n",
        "\n",
        "            const initialCommit = getCurrentCommit();\n",
        "            const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n",
        "            if (button) button.click();\n",
        "\n",
        "            // Poll for changes\n",
        "            while (true) {\n",
        "                await new Promise(resolve => setTimeout(resolve, 100)); // Wait 100ms\n",
        "                const newCommit = getCurrentCommit();\n",
        "                if (newCommit && newCommit !== initialCommit) {\n",
        "                    break;\n",
        "                }\n",
        "            }\n",
        "        })();\n",
        "        \"\"\"\n",
        "\n",
        "        schema = {\n",
        "            \"name\": \"Commit Extractor\",\n",
        "            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n",
        "            \"fields\": [\n",
        "                {\n",
        "                    \"name\": \"title\",\n",
        "                    \"selector\": \"h4.markdown-title\",\n",
        "                    \"type\": \"text\",\n",
        "                    \"transform\": \"strip\",\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "        for page in range(2):  # Crawl 2 pages\n",
        "            config = CrawlerRunConfig(\n",
        "                cache_mode=CacheMode.BYPASS,\n",
        "                session_id=session_id,\n",
        "                css_selector=\"li.Box-sc-g0xbh4-0\",\n",
        "                extraction_strategy=extraction_strategy,\n",
        "                js_code=js_next_page_and_wait if page > 0 else None,\n",
        "                js_only=page > 0,\n",
        "            )\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            assert result.success, f\"Failed to crawl page {page + 1}\"\n",
        "\n",
        "            commits = json.loads(result.extracted_content)\n",
        "            all_commits.extend(commits)\n",
        "\n",
        "            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n",
        "\n",
        "        await crawler.crawler_strategy.kill_session(session_id)\n",
        "        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n",
        "\n",
        "asyncio.run(crawl_dynamic_content_pages_method_2())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. **Using Extraction Strategies**"
      ],
      "metadata": {
        "id": "v6myEZ6Qhwtq"
      },
      "id": "v6myEZ6Qhwtq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Executing JavaScript & Extract Structured Data without LLMs"
      ],
      "metadata": {
        "id": "Y1gKhtzph4NV"
      },
      "id": "Y1gKhtzph4NV"
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.extraction_strategy import (\n",
        "    JsonCssExtractionStrategy,\n",
        "    LLMExtractionStrategy,\n",
        ")\n",
        "import json\n",
        "async def extract():\n",
        "    schema = {\n",
        "        \"name\": \"KidoCode Courses\",\n",
        "        \"baseSelector\": \"section.charge-methodology .div-block-214.p-extraxx\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"section_title\",\n",
        "                \"selector\": \"h3.heading-50\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"section_description\",\n",
        "                \"selector\": \".charge-content\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"course_name\",\n",
        "                \"selector\": \".text-block-93\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"course_description\",\n",
        "                \"selector\": \".course-content-text\",\n",
        "                \"type\": \"text\",\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"course_icon\",\n",
        "                \"selector\": \".image-92\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "\n",
        "        # Create the JavaScript that handles clicking multiple times\n",
        "        js_click_tabs = \"\"\"\n",
        "        (async () => {\n",
        "            const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n",
        "\n",
        "            for(let tab of tabs) {\n",
        "                // scroll to the tab\n",
        "                tab.scrollIntoView();\n",
        "                tab.click();\n",
        "                // Wait for content to load and animations to complete\n",
        "                await new Promise(r => setTimeout(r, 500));\n",
        "            }\n",
        "        })();\n",
        "        \"\"\"\n",
        "\n",
        "        config = CrawlerRunConfig(\n",
        "            cache_mode=CacheMode.BYPASS,\n",
        "            extraction_strategy=extraction_strategy,\n",
        "            js_code=[js_click_tabs],\n",
        "        )\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.kidocode.com/degrees/technology\",\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        courses = json.loads(result.extracted_content)\n",
        "        print(result.extracted_content)\n",
        "        print(f\"Successfully extracted {len(courses)} courses\")\n",
        "        print(len(result.markdown))\n",
        "        # print(json.dumps(courses[0], indent=2))\n",
        "\n",
        "await extract()"
      ],
      "metadata": {
        "id": "QZk5kW6rhzTO"
      },
      "id": "QZk5kW6rhzTO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad32a778",
      "metadata": {
        "id": "ad32a778"
      },
      "source": [
        "#####  LLM Extraction\n",
        "\n",
        "This example demonstrates how to use language model-based extraction to retrieve structured data from a pricing page on OpenAI‚Äôs site."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3011a7c5",
      "metadata": {
        "id": "3011a7c5"
      },
      "outputs": [],
      "source": [
        "from crawl4ai.extraction_strategy import LLMExtractionStrategy\n",
        "from pydantic import BaseModel, Field\n",
        "import os, json\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "class OpenAIModelFee(BaseModel):\n",
        "    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n",
        "    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n",
        "    output_fee: str = Field(\n",
        "        ..., description=\"Fee for output token for the OpenAI model.\"\n",
        "    )\n",
        "\n",
        "async def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):\n",
        "    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n",
        "\n",
        "    # Skip if API token is missing (for providers that require it)\n",
        "    if api_token is None and provider != \"ollama\":\n",
        "        print(f\"API token is required for {provider}. Skipping this example.\")\n",
        "        return\n",
        "\n",
        "    extra_args = {\"extra_headers\": extra_headers} if extra_headers else {}\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://openai.com/api/pricing/\",\n",
        "            word_count_threshold=1,\n",
        "            extraction_strategy=LLMExtractionStrategy(\n",
        "                provider=provider,\n",
        "                api_token=api_token,\n",
        "                schema=OpenAIModelFee.schema(),\n",
        "                extraction_type=\"schema\",\n",
        "                instruction=\"\"\"Extract all model names along with fees for input and output tokens.\"\n",
        "                \"{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}.\"\"\",\n",
        "                **extra_args\n",
        "            ),\n",
        "            cach_mode = CacheMode.ENABLED\n",
        "        )\n",
        "        print(json.loads(result.extracted_content)[:5])\n",
        "\n",
        "# Usage:\n",
        "# await extract_structured_data_using_llm(\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\", os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
        "# await extract_structured_data_using_llm(\"ollama/llama3.2\")\n",
        "await extract_structured_data_using_llm(\"openai/gpt-4o-mini\", os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6532db9d",
      "metadata": {
        "id": "6532db9d"
      },
      "source": [
        "**Cosine Similarity Strategy**\n",
        "\n",
        "This strategy uses semantic clustering to extract relevant content based on contextual similarity, which is helpful when extracting related sections from a single topic.\n",
        "\n",
        "IMPORTANT: This strategy uses embedding models from HuggingFace, to have a proper response time, make sure to switch to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec079108",
      "metadata": {
        "id": "ec079108"
      },
      "outputs": [],
      "source": [
        "from crawl4ai.extraction_strategy import CosineStrategy\n",
        "\n",
        "async def cosine_similarity_extraction():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        strategy = CosineStrategy(\n",
        "            word_count_threshold=10,\n",
        "            max_dist=0.2, # Maximum distance between two words\n",
        "            linkage_method=\"ward\", # Linkage method for hierarchical clustering (ward, complete, average, single)\n",
        "            top_k=3, # Number of top keywords to extract\n",
        "            sim_threshold=0.3, # Similarity threshold for clustering\n",
        "            semantic_filter=\"McDonald's economic impact, American consumer trends\", # Keywords to filter the content semantically using embeddings\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\",\n",
        "            extraction_strategy=strategy,\n",
        "            cach_mode = CacheMode.ENABLED\n",
        "        )\n",
        "        print(json.loads(result.extracted_content)[:5])\n",
        "\n",
        "asyncio.run(cosine_similarity_extraction())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff423629",
      "metadata": {
        "id": "ff423629"
      },
      "source": [
        "#### 10. **Conclusion and Next Steps**\n",
        "\n",
        "You‚Äôve explored core features of Crawl4AI, including dynamic content handling, link analysis, and advanced extraction strategies. Visit our documentation for further details on using Crawl4AI‚Äôs extensive features.\n",
        "\n",
        "- GitHub Repository: [https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n",
        "- Twitter: [@unclecode](https://twitter.com/unclecode)\n",
        "- Website: [https://crawl4ai.com](https://crawl4ai.com)\n",
        "\n",
        "Happy Crawling with Crawl4AI! üï∑Ô∏èü§ñ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d34c1d35",
      "metadata": {
        "id": "d34c1d35"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}